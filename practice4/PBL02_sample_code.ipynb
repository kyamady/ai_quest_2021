{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PBL02_不良個所自動検出 良否判定モデル構築用サンプルコード\n",
    "\n",
    "本jupyter notebookはPBL02_不良個所自動検出のサンプルコードです。<br>\n",
    "**で囲まれた箇所をご自身の環境に合わせて変更いただいた上で本jupyter notebookを上から下まで実行いただけると提出可能なファイルが出力されます。<br>\n",
    "<br>\n",
    "本jupyter notebookの構成は以下のようになっております。<br>\n",
    "<ol>\n",
    "<li>ライブラリimport</li>\n",
    "    必要なライブラリのimportを行う。\n",
    "<li>パラメータ設定</li>\n",
    "    画像分類アルゴリズム\"VGG\"に関する設定とデータ、ウェイトのパス設定を行う。\n",
    "<li>VGGのネットワーク定義</li>\n",
    "    VGGのネットワークを本課題向けにカスタマイズし、カスタマイズしたVGGを宣言する。<br>\n",
    "    具体的には、VGGの基本設定、事前重み有無設定、出力層のカスタマイズを行う。\n",
    "<li>学習・検証データの読み込み</li>\n",
    "    2. で指定した格納先の学習・検証データを読み込む。\n",
    "<li>モデルの学習</li>\n",
    "    読み込んだデータを用いてVGGを学習させる。\n",
    "<li>モデルによる判定</li>\n",
    "    構築したモデルによる判定を実施する。\n",
    "<li>学習・検証データに対する精度評価</li>\n",
    "    学習・検証データに対する精度をf1 scoreで評価する。\n",
    "<li>提出ファイルの出力</li>\n",
    "    テストデータに対して良否判定を行い、その結果を提出フォーマットであるtsv形式で出力を行う。\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ライブラリimport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-1a96c77eccb0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")    \n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D, Input\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras import backend as K\n",
    "from keras import optimizers\n",
    "from keras.utils import np_utils\n",
    "from keras.applications.vgg16 import VGG16\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(1)\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. パラメータ設定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "画像分類アルゴリズム\"VGG\"に関する設定とデータとウェイトのパス設定を行います。<br>\n",
    "次セルの「画像分類アルゴリズム\"VGG\"に関する設定」でパラメータを変更することにより、精度向上が期待できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 画像分類アルゴリズム\"VGG\"に関する設定\n",
    "# 入力画像サイズの高さと幅\n",
    "IMG_WIDTH, IMG_HEIGHT = 224, 224\n",
    "TARGET_SIZE = (IMG_WIDTH, IMG_HEIGHT)\n",
    "# 判定分類数（4分類を判定するモデルを構築し、そのモデルの判別結果を最後に良品、不良品の2分類に変換する前提）\n",
    "NB_CLASSES = 4\n",
    "# 学習時のエポック数\n",
    "EPOCHS = 30\n",
    "# バッチサイズ\n",
    "BATCH_SIZE = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if K.image_data_format() == 'channels_first':\n",
    "    input_shape = (3, IMG_WIDTH, IMG_HEIGHT)\n",
    "else:\n",
    "    input_shape = (IMG_WIDTH, IMG_HEIGHT, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データとウェイトに関する設定\n",
    "train_data_dir = '*任意の学習用データ保存先*'　# 学習データ保存先にはbridge, horn, potato, regularのフォルダがあり、各フォルダ配下に画像が格納されている想定\n",
    "validation_data_dir = '*任意の検証用データ保存先*'　# 検証データ保存先にはbridge, horn, potato, regularのフォルダがあり、各フォルダ配下に画像が格納されている想定\n",
    "test_data_dir = '*任意のテスト用データ保存先*'　# テストデータ保存先には画像データが格納されている想定\n",
    "weight_dir = '*任意のweight保存先*'    \n",
    "save_weights_path = os.path.join(weight_dir, '*任意のweightファイル名*') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. VGGのネットワーク定義"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VGGのネットワークを本課題向けにカスタマイズし、カスタマイズしたVGGを宣言します。<br>\n",
    "具体的には、VGGの基本設定、事前重み有無設定、出力層のカスタマイズなどを行います。<br>\n",
    "「VGGの学習方法の定義」セルにてoptimizerの種類と学習率を変更することにより、精度向上が期待できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGGの基本設定\n",
    "base_model = VGG16(\n",
    "    weights='imagenet',\n",
    "    include_top=False,\n",
    "    input_tensor=Input(shape=(IMG_WIDTH, IMG_HEIGHT, 3))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ネットワーク構造の確認\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 出力に近い層を本課題に合わせて変更\n",
    "top_model = base_model.output\n",
    "top_model = Flatten(name='flatten')(top_model)\n",
    "top_model = Dense(512, activation='relu')(top_model)\n",
    "top_model = Dropout(0.5)(top_model)\n",
    "top_model = Dense(NB_CLASSES, activation='softmax')(top_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# カスタマイズ後のVGGの定義\n",
    "model = Model(\n",
    "    inputs=base_model.input,\n",
    "    outputs=top_model\n",
    ")\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  VGGの学習方法の定義\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=optimizers.RMSprop(lr=1e-4), #optimizerの種類（\"RMSprop\"の箇所）と学習率（\"lr\"の箇所）を変更することにより、精度向上が期待できます。\n",
    "    metrics=['accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ネットワーク構造の確認\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 学習・検証データの読み込み"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.で指定した格納先の学習・検証データを読み込みます。<br>\n",
    "読み込み前、読み込み時にこれらのデータに前処理（特徴量の強調、クラス分布の平準化、データの分布平準化）を施すことで精度向上が期待できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGGに入力できるよう画像サイズの圧縮\n",
    "train_datagen = ImageDataGenerator(rescale=1.0/255) # 前処理を（）内に追加可能\n",
    "valid_datagen = ImageDataGenerator(rescale=1.0/255) # 前処理を（）内に追加可能\n",
    "# test_datagen = ImageDataGenerator(rescale=1.0/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#学習・検証データの読み込み\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=TARGET_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "validation_generator = valid_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=TARGET_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習・検証データとして上記にて読み込んだ画像を設定\n",
    "nb_train_samples = train_generator.samples\n",
    "nb_validation_samples = validation_generator.samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. モデルの学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "読み込んだデータを用いてVGGを学習させます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=nb_train_samples/BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=nb_validation_samples/BATCH_SIZE\n",
    ")\n",
    "model.save_weights(save_weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# loss curveの表示\n",
    "plt.figure(figsize=[10,8])\n",
    "plt.plot(model.history.history['loss'], 'r')\n",
    "plt.plot(model.history.history['val_loss'], 'b')\n",
    "plt.legend(['Training loss', 'Validation Loss'])\n",
    "plt.xlabel('Epochs', fontsize=16)\n",
    "plt.ylabel('Loss', fontsize=16)\n",
    "plt.title('Loss Curves', fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# accuracy curveの表示\n",
    "plt.figure(figsize=[10,8])\n",
    "plt.plot(model.history.history['acc'], 'r')\n",
    "plt.plot(model.history.history['val_acc'], 'b')\n",
    "plt.legend(['Training Accuracy', 'Validation Accuracy'])\n",
    "plt.xlabel('Epochs', fontsize=16)\n",
    "plt.ylabel('Accuracy', fontsize=16)\n",
    "plt.title('Accuracy Curves', fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. モデルによる判定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "構築したモデルによる判定を行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weightファイルの読み込み\n",
    "print('load model...')\n",
    "model.load_weights(save_weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 良否判定実行関数\n",
    "def get_predict(model,\n",
    "                train_data_dir: str,\n",
    "                test_data_dir: str):\n",
    "    \"\"\"This function will performs model inferencing using test data\n",
    "    and stores the results into the lists.\n",
    "    \n",
    "    Args:\n",
    "        model (object): The trained model.\n",
    "        train_data_dir (str): The location of train images.\n",
    "        test_data_dir (str): The location of test images.\n",
    "        \n",
    "    Returns:\n",
    "        filenames (list): filenames of predicted images.\n",
    "        true_classes (list): true classes of predicted images.\n",
    "        pred_classes (list): prediction classes of predicted images.\n",
    "    \"\"\"\n",
    "    \n",
    "    data_datagen = ImageDataGenerator(rescale=1/255.)\n",
    "\n",
    "    test_generator = data_datagen.flow_from_directory(\n",
    "        test_data_dir,\n",
    "        target_size=TARGET_SIZE,\n",
    "        class_mode=None,\n",
    "        batch_size=1,\n",
    "        shuffle=False,\n",
    "    )\n",
    "    preds = model.predict_generator(test_generator)\n",
    "\n",
    "    preds_class_idx = preds.argmax(axis=-1)\n",
    "    \n",
    "    # get prediction class\n",
    "    train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "    \n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=TARGET_SIZE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "    )\n",
    "    \n",
    "    idx_to_class = {v: k for k, v in train_generator.class_indices.items()}\n",
    "    pred_classes = np.vectorize(idx_to_class.get)(preds_class_idx)\n",
    "    filenames_to_class = list(zip(test_generator.filenames, pred_classes))\n",
    "    \n",
    "    # get true class\n",
    "    filenames = []\n",
    "    true_classes = []\n",
    "\n",
    "    for item in test_generator.filenames:\n",
    "        filenames.append(item)\n",
    "        # get true class from the filenames\n",
    "        true_class = item.split('/')[0]\n",
    "        true_classes.append(true_class)\n",
    "    \n",
    "    return filenames, true_classes, pred_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 精度算出関数\n",
    "def get_f1(true_labels_list: list,\n",
    "           predictions_list: list,\n",
    "           average_method: str,\n",
    "          ) -> (float, float, float):\n",
    "    \"\"\"This function will performs model inferencing using test data\n",
    "    and stores the results into the lists.\n",
    "    \n",
    "    Args:\n",
    "        true_labels_list (list): List of true labels.\n",
    "        predictions_list (list): List of predictions.\n",
    "        average_method (string): method to average score.\n",
    "        \n",
    "    Returns:\n",
    "        f1 (float): return f1 metric.\n",
    "        precision (float): return precision metric.\n",
    "        recall (float): return recall metric.\n",
    "    \"\"\"\n",
    "    f1 = f1_score(\n",
    "        y_true=true_labels_list,\n",
    "        y_pred=predictions_list,\n",
    "        average=average_method\n",
    "    )\n",
    "    \n",
    "    precision = precision_score(\n",
    "        y_true=true_labels_list,\n",
    "        y_pred=predictions_list,\n",
    "        average=average_method,\n",
    "    )\n",
    "    \n",
    "    recall = recall_score(\n",
    "        y_true=true_labels_list,\n",
    "        y_pred=predictions_list,\n",
    "        average=average_method,\n",
    "    )\n",
    "    \n",
    "    f1 = round(f1, 2)\n",
    "    precision = round(precision, 2)\n",
    "    recall = round(recall, 2)\n",
    "    \n",
    "    return f1, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 良否判定実行\n",
    "train_filenames, train_true_classes, train_pred_classes = get_predict(\n",
    "    model=model,\n",
    "    train_data_dir=train_data_dir,\n",
    "    test_data_dir=train_data_dir,\n",
    ")\n",
    "valid_filenames, valid_true_classes, valid_pred_classes = get_predict(\n",
    "    model=model,                                                        \n",
    "    train_data_dir=train_data_dir,                                                            \n",
    "    test_data_dir=validation_data_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 学習・検証データに対する精度評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 精度算出\n",
    "train_f1, train_prec, train_recall = get_f1(\n",
    "    true_labels_list=train_true_classes,\n",
    "    predictions_list=train_pred_classes,\n",
    "    average_method='weighted',\n",
    ")\n",
    "valid_f1, valid_prec, valid_recall = get_f1(\n",
    "    true_labels_list=valid_true_classes,\n",
    "    predictions_list=valid_pred_classes,\n",
    "    average_method='weighted',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 精度表示\n",
    "print('{:15}{:<15.2f}{:<15.2f}'.format('F1-score:', train_f1, valid_f1))\n",
    "print('{:15}{:<15.2f}{:<15.2f}'.format('Precision:', train_prec, valid_prec))\n",
    "print('{:15}{:<15.2f}{:<15.2f}'.format('Recall:', train_recall, valid_recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 提出ファイルの出力"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "テストデータに対して良否判定を行い、その結果を提出フォーマットであるtsv形式で出力を行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分類とラベルの対応確認\n",
    "label_map = (train_generator.class_indices)\n",
    "print(label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テストデータに対して1つずつ予測し、テストデータのファイル名と判定結果をリストに保存\n",
    "file_list = []\n",
    "pred_list = []\n",
    "for file in glob.glob(test_data_dir + '/*'):\n",
    "    image_data = file\n",
    "    filename = file.split('/')[-1]\n",
    "    img = image.load_img(image_data, target_size=(IMG_WIDTH, IMG_HEIGHT))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = x / 255\n",
    "    pred = model.predict(x)[0]\n",
    "    judge = np.argmax(pred)\n",
    "\n",
    "    # *bridge, horn, potatoを不良（'1'）に、regularを良品（'0'）に変換。if文の条件分岐は上の「分類とラベルの対応確認」セルの結果を参考に変更すること*\n",
    "    if judge==0:\n",
    "        judge=1\n",
    "    elif judge==1:\n",
    "        judge=1\n",
    "    elif judge==2:\n",
    "        judge=1\n",
    "    else:\n",
    "        judge=0\n",
    "\n",
    "    pred_list.append(judge)\n",
    "    file_list.append(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#判別結果をDataFrameに変換し、tsvファイルに出力\n",
    "df = pd.DataFrame([file_list, pred_list]).T\n",
    "df.to_csv('my_submission.tsv',\n",
    "         index=False,\n",
    "         header=False,\n",
    "         sep='\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
